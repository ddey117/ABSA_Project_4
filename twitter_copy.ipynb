{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/modifier_aspect_unlabeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_turk = df2.loc[:, ['Input.Modifier', 'Input.Aspect',  'Answer.sentiment.label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_turk, \n",
    "#          df, \n",
    "#          left_on=['Input.Modifier', 'Input.Aspect'],\n",
    "#          right_on=['modifier', 'asp'],\n",
    "#          how='inner'\n",
    "#         )# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_aspect_sentiment_tuples(df):\n",
    "#     df = master_preprocess(df)\n",
    "#     df = df.loc[df['wordcounts'] > 10].copy()\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     df['aspect_tups'] = df.apply(apply_extraction, axis=1)\n",
    "#     df = df.explode('aspect_tups').copy()\n",
    "#     df.dropna(inplace=True)\n",
    "#     df['asp'] = df['aspect_tups'].apply(lambda x: x[0])\n",
    "#     df['modifier'] = df['aspect_tups'].apply(lambda x: x[1])\n",
    "#     df['modifier_sentiment'] = df['aspect_tups'].apply(lambda x: x[2])\n",
    "#     df['rule_number'] = df['aspect_tups'].apply(lambda x: x[3])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def get_cluster_name_inputs(df):\n",
    "#     print('loading....')\n",
    "#     aspect_freq_map = get_aspect_freq_map(df['asp'].values)\n",
    "#     unique_asp_array = df['asp'].unique()\n",
    "#     mapped_labels = get_word_cluster_labels(unique_asp_array)\n",
    "#     asp_labels_map = dict(zip(unique_asp_array, mapped_labels))\n",
    "#     label_names_map = get_cluster_names_map(asp_labels_map, aspect_freq_map)\n",
    "    \n",
    "#     df['asp_cluster_label'] = df['asp'].map(asp_labels_map)\n",
    "    \n",
    "#     print(\"write misc if low counts and special characters\")\n",
    "#     print(\"the top word is usually the best fit\")\n",
    "   \n",
    "#     display(label_names_map[0][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_0 = input()\n",
    "\n",
    "#     display(label_names_map[1][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_1 = input()\n",
    "\n",
    "#     display(label_names_map[2][:10])\n",
    "  \n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_2 = input()\n",
    "\n",
    "#     display(label_names_map[3][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_3 = input()\n",
    "\n",
    "#     clusters = [clust_0] + [clust_1] + [clust_2] + [clust_3]\n",
    "\n",
    "\n",
    "#     name_clust_dict = {0: clusters[0],\n",
    "#                        1: clusters[1],\n",
    "#                        2: clusters[2],\n",
    "#                        3: clusters[3]\n",
    "#                       }\n",
    "\n",
    "    \n",
    "\n",
    "#     df['cluster_name'] = df['asp_cluster_label'].map(name_clust_dict)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect_list = []\n",
    "# for row in range(0,500):\n",
    "#     aspect = apply_extraction(df3.iloc[row], nlp=nlp, sid=sid)\n",
    "#     aspect_list.append(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_digits(x):\n",
    "#     return \" \".join([t for t in x.split() if not t.isdigit()])\n",
    "\n",
    "# def get_aspect_freq_map(aspects):\n",
    "#     aspect_freq_map = defaultdict(int)\n",
    "#     for asp in aspects:\n",
    "#         aspect_freq_map[asp] += 1\n",
    "#     return aspect_freq_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = master_preprocess(df2)\n",
    "# df2 = df2.loc[df2['wordcounts'] > 10].copy()\n",
    "# df2.reset_index(drop=True, inplace=True)\n",
    "# df2['aspect_tups'] = df2.apply(apply_extraction, axis=1)\n",
    "\n",
    "\n",
    "# df_hp1 = master_preprocess(df_hp1)\n",
    "# df_hp2 = master_preprocess(df_hp2)\n",
    "# df_hp3 = master_preprocess(df_hp3)\n",
    "# df_hp4 = master_preprocess(df_hp4)\n",
    "# df_sb1 = master_preprocess(df_sb1)\n",
    "# df_sb2 = master_preprocess(df_sb2)\n",
    "# df_mp1 = master_preprocess(df_mp1)\n",
    "# df_mp2 = master_preprocess(df_mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti = extract_aspect_sentiment_tuples(df2)\n",
    "# df_senti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti[['modifier', 'asp']].to_csv(\"data/modifier_aspect_unlabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti = get_cluster_name_inputs(df_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_clusters = df_senti.groupby(['cluster_name', 'star_rating'])['star_rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti.loc[df_senti['modifier_sentiment'] == 0, 'polarity'] = 'neutral'\n",
    "# df_senti.loc[df_senti['modifier_sentiment'] > 0, 'polarity'] = 'positive'\n",
    "# df_senti.loc[df_senti['modifier_sentiment'] < 0, 'polarity'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti2 = df_senti.loc[df_senti['modifier_sentiment'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity_clusters = df_senti2.groupby(['cluster_name', 'polarity'])['polarity'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,8))\n",
    "# polarity_clusters.plot(kind='bar')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_clusters = df_senti2.groupby(['cluster_name', 'star_rating'])['star_rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,8))\n",
    "# rating_clusters.plot(kind='bar')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti2.loc[(df_senti2['modifier_sentiment'] > -0.5) & (df_senti2['modifier_sentiment'] < 0), 'polarity'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_senti2.loc[df_senti2['star_rating'] == 1, 'polarity'] = 'very_negative'\n",
    "# df_senti2.loc[df_senti2['star_rating']== 2, 'polarity'] = 'negative'\n",
    "# df_senti2.loc[df_senti2['star_rating'] == 3, 'polarity'] = 'neutral'\n",
    "# df_senti2.loc[df_senti2['star_rating'] == 4, 'polarity'] = 'positive'\n",
    "# df_senti2.loc[df_senti2['star_rating']== 5, 'polarity'] = 'very_positive'\n",
    "\n",
    "# df_senti2.loc[df_senti2['modifier_sentiment']  <= -0.5, 'sentiment'] = 'very_negative'\n",
    "# df_senti2.loc[(df_senti2['modifier_sentiment'] > -0.5) & (df_senti2['modifier_sentiment'] < 0), 'sentiment'] = 'negative'\n",
    "# df_senti2.loc[df_senti2['modifier_sentiment'] == 0, 'sentiment'] = 'neutral'\n",
    "# df_senti2.loc[(df_senti2['modifier_sentiment'] > 0) & (df_senti2['modifier_sentiment'] < 0.5), 'sentiment'] = 'positive'\n",
    "# df_senti2.loc[df_senti2['modifier_sentiment'] >= 0.5, 'sentiment'] = 'very_positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senti2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import urllib.request\n",
    "# import gzip\n",
    "# import sys\n",
    "# import spacy\n",
    "# import json\n",
    "# import boto3\n",
    "# from boto.s3.connection import S3Connection\n",
    "# import preprocess_ddey117 as pp\n",
    "# from collections import defaultdict\n",
    "# from sklearn import cluster\n",
    "# import seaborn as sns\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# import nltk\n",
    "# # nltk.download('vader_lexicon')\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # BASE_PATH = os.getcwd()\n",
    "# # PARENT = os.path.dirname(BASE_PATH)\n",
    "\n",
    "\n",
    "# # #USE THIS FOR IMPORTING ANY FUNCTIONS FROM src\n",
    "# # sys.path.insert(0,BASE_PATH)\n",
    "\n",
    "\n",
    "# # prod_pronouns = ['it','this','they','these']\n",
    "\n",
    "# # def fetch_reviews(filepath):\n",
    "# #     raw_data = pd.read_table(filepath,nrows=300,error_bad_lines=False) #nrows = 300\n",
    "# #     return raw_data\n",
    "\n",
    "# # def fetch_s3(filename):\n",
    "# #     s3_basepath = 's3://amazon-reviews-pds/tsv/'\n",
    "# #     s3_fullpath = s3_basepath + filename\n",
    "# #     raw_data = pd.read_table(s3_fullpath, compression = 'gzip',error_bad_lines=False)\n",
    "# #     return raw_data\n",
    "\n",
    "# # df_electronics = fetch_s3(\"amazon_reviews_us_Electronics_v1_00.tsv.gz\")\n",
    "\n",
    "# # df_electronics.to_csv('data/df_electronics.tsv', sep='\\t', index=False)\n",
    "\n",
    "# df = pd.read_csv('data/df_electronics.tsv', sep='\\t')\n",
    "\n",
    "# df.groupby('product_id').count().sort_values(by='star_rating').tail(30)\n",
    "\n",
    "# df2 = df.loc[df['product_id'] == 'B0001FTVEK'].copy()\n",
    "# df2.reset_index(drop=True, inplace=True)\n",
    "# df_not_2 = df.loc[df['product_id'] != 'B0001FTVEK'].copy()\n",
    "# df_not_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # df_hp1 = df.loc[df['product_id'] == 'B003EM8008'].copy()\n",
    "# # df_hp1.reset_index(drop=True, inplace=True)\n",
    "# # df_hp2 = df.loc[df['product_id'] == 'B0001FTVEK'].copy()\n",
    "# # df_hp2.reset_index(drop=True, inplace=True)\n",
    "# # df_hp3 = df.loc[df['product_id'] == 'B004RKQM8I'].copy()\n",
    "# # df_hp3.reset_index(drop=True, inplace=True)\n",
    "# # df_hp4 = df.loc[df['product_id'] == 'B0038W0K2K'].copy()\n",
    "# # df_hp4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # df_sb1 = df.loc[df['product_id'] == 'B00D5Q75RC'].copy()\n",
    "# # df_sb1.reset_index(drop=True, inplace=True)\n",
    "# # df_sb2 = df.loc[df['product_id'] == 'B00F5NE2KG'].copy()\n",
    "# # df_sb2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # df_mp1 = df.loc[df['product_id'] == 'B00020S7XK'].copy()\n",
    "# # df_mp1.reset_index(drop=True, inplace=True)\n",
    "# # df_mp2 = df.loc[df['product_id'] == 'B002MAPT7U'].copy()\n",
    "# # df_mp2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df2.columns\n",
    "\n",
    "# columns_td = ['marketplace', 'customer_id',  'product_id',\n",
    "#               'product_parent', 'product_title', 'product_category', \n",
    "#               'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
    "#               'review_headline', 'review_date']\n",
    "\n",
    "# df2.drop(columns=columns_td, inplace=True)\n",
    "\n",
    "# # df_hp1.drop(columns=columns_td, inplace=True)\n",
    "# # df_hp2.drop(columns=columns_td, inplace=True)\n",
    "# # df_hp3.drop(columns=columns_td, inplace=True)\n",
    "# # df_hp4.drop(columns=columns_td, inplace=True)\n",
    "# # df_sb1.drop(columns=columns_td, inplace=True)\n",
    "# # df_sb2.drop(columns=columns_td, inplace=True)\n",
    "# # df_mp1.drop(columns=columns_td, inplace=True)\n",
    "# # df_mp2.drop(columns=columns_td, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # \"http://www.amazon.com/exec/obidos/ASIN/\" + \"test\"\n",
    "\n",
    "# # def add_amazonlink(product_id):\n",
    "# #     url = \"http://www.amazon.com/exec/obidos/ASIN/\" + product_id\n",
    "# #     return url\n",
    "\n",
    "# # add_amazonlink(\"B00JOQIO6S\")\n",
    "\n",
    "# def apply_extraction(row,nlp=nlp,sid=sid):\n",
    "#     review_body = row['review_body']\n",
    "# #     review_id = row['review_id']\n",
    "# #     review_marketplace = row['marketplace']\n",
    "# #     customer_id = row['customer_id']\n",
    "# #     product_id = row['product_id']\n",
    "# #     product_parent = row['product_parent']\n",
    "# #     product_title = row['product_title']\n",
    "# #     product_category = row['product_category']\n",
    "# #     date = str(row['review_date'])\n",
    "# #     star_rating = row['star_rating']\n",
    "# #     url = add_amazonlink(product_id)\n",
    "\n",
    "\n",
    "\n",
    "#     doc=nlp(review_body)\n",
    "\n",
    "\n",
    "#     ## FIRST RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "#     ## RULE = M is child of A with a relationshio of amod\n",
    "#     ner_heads = {ent.root.idx: ent for ent in doc.ents}\n",
    "#     rule1_pairs = []\n",
    "#     for token in doc:\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         if token.dep_ == \"amod\" and not token.is_stop:\n",
    "#             M = token.text\n",
    "#             if token.head in ner_heads:\n",
    "#                 A = ner_heads[token.head].text\n",
    "#             else:\n",
    "#                 A = token.head.text\n",
    "\n",
    "#             # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n",
    "#             M_children = token.children\n",
    "#             for child_m in M_children:\n",
    "#                 if(child_m.dep_ == \"advmod\"):\n",
    "#                     M_hash = child_m.text\n",
    "#                     M = M_hash + \" \" + M\n",
    "#                     break\n",
    "\n",
    "#             # negation in adjective, the \"no\" keyword is a 'det' of the noun (e.g. no interesting characters)\n",
    "#             A_children = token.head.children\n",
    "#             for child_a in A_children:\n",
    "#                 if(child_a.dep_ == \"det\" and child_a.text == 'no'):\n",
    "#                     neg_prefix = 'not'\n",
    "#                     M = neg_prefix + \" \" + M\n",
    "#                     break\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule1_pairs.append((A, M,sid.polarity_scores(token.text)['compound'],1))\n",
    "\n",
    "#     ## SECOND RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "#     #Direct Object - A is a child of something with relationship of nsubj, while\n",
    "#     # M is a child of the same something with relationship of dobj\n",
    "#     #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "# #     ner_heads = {ent.root.idx: ent for ent in doc.ents}\n",
    "#     rule2_pairs = []\n",
    "#     for token in doc:\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         add_neg_pfx = False\n",
    "#         for child in children :\n",
    "#             if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "#                 if child.idx in ner_heads:\n",
    "#                     A = ner_heads[child.idx].text\n",
    "#                 else:\n",
    "#                     A = child.text\n",
    "#                 # check_spelling(child.text)\n",
    "\n",
    "#             if((child.dep_ == \"dobj\" and child.pos_ == \"ADJ\") and not child.is_stop):\n",
    "#                 M = child.text\n",
    "#                 #check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"neg\"):\n",
    "#                 neg_prefix = child.text\n",
    "#                 add_neg_pfx = True\n",
    "\n",
    "#     if (add_neg_pfx and M != \"999999\"):\n",
    "#         M = neg_prefix + \" \" + M\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule2_pairs.append((A, M,sid.polarity_scores(M)['compound'],2))\n",
    "\n",
    "\n",
    "#     ## THIRD RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "#     ## Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
    "#     ## M is a child of the same something with relationship of acomp\n",
    "#     ## Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "#     ## \"The sound of the speakers would be better. The sound of the speakers could be better\" - handled using AUX dependency\n",
    "\n",
    "\n",
    "#     rule3_pairs = []\n",
    "\n",
    "#     for token in doc:\n",
    "\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         add_neg_pfx = False\n",
    "#         for child in children :\n",
    "#             if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "#                 if child.idx in ner_heads:\n",
    "#                     A = ner_heads[child.idx].text\n",
    "#                 else:\n",
    "#                     A = child.text\n",
    "#                 # check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"acomp\" and not child.is_stop):\n",
    "#                 M = child.text\n",
    "\n",
    "#             # example - 'this could have been better' -> (this, not better)\n",
    "#             if(child.dep_ == \"aux\" and child.tag_ == \"MD\"):\n",
    "#                 neg_prefix = \"not\"\n",
    "#                 add_neg_pfx = True\n",
    "\n",
    "#             if(child.dep_ == \"neg\"):\n",
    "#                 neg_prefix = child.text\n",
    "#                 add_neg_pfx = True\n",
    "\n",
    "#         if (add_neg_pfx and M != \"999999\"):\n",
    "#             M = neg_prefix + \" \" + M\n",
    "#                 #check_spelling(child.text)\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule3_pairs.append((A, M, sid.polarity_scores(M)['compound'],3))\n",
    "\n",
    "#     ## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#     #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
    "#     # M is a child of the same something with relationship of advmod\n",
    "\n",
    "#     #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "#     rule4_pairs = []\n",
    "#     for token in doc:\n",
    "\n",
    "\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         add_neg_pfx = False\n",
    "#         for child in children :\n",
    "#             if((child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop):\n",
    "#                 if child.idx in ner_heads:\n",
    "#                     A = ner_heads[child.idx].text\n",
    "#                 else:\n",
    "#                     A = child.text\n",
    "#                 # check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"advmod\" and not child.is_stop):\n",
    "#                 M = child.text\n",
    "#                 M_children = child.children\n",
    "#                 for child_m in M_children:\n",
    "#                     if(child_m.dep_ == \"advmod\"):\n",
    "#                         M_hash = child_m.text\n",
    "#                         M = M_hash + \" \" + child.text\n",
    "#                         break\n",
    "#                 #check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"neg\"):\n",
    "#                 neg_prefix = child.text\n",
    "#                 add_neg_pfx = True\n",
    "\n",
    "#         if (add_neg_pfx and M != \"999999\"):\n",
    "#             M = neg_prefix + \" \" + M\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule4_pairs.append((A, M,sid.polarity_scores(M)['compound'],4)) # )\n",
    "\n",
    "\n",
    "#     ## FIFTH RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "#     #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
    "#     # M has a child with relationship of cop\n",
    "\n",
    "#     #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "#     rule5_pairs = []\n",
    "#     for token in doc:\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         buf_var = \"999999\"\n",
    "#         for child in children :\n",
    "#             if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "#                 if child.idx in ner_heads:\n",
    "#                     A = ner_heads[child.idx].text\n",
    "#                 else:\n",
    "#                     A = child.text\n",
    "                \n",
    "#                 # check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"cop\" and not child.is_stop):\n",
    "#                 buf_var = child.text\n",
    "#                 #check_spelling(child.text)\n",
    "\n",
    "#         if(A != \"999999\" and buf_var != \"999999\"):\n",
    "#             rule5_pairs.append((A, token.text,sid.polarity_scores(token.text)['compound'],5))\n",
    "\n",
    "\n",
    "#     ## SIXTH RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "#     ## Example - \"It ok\", \"ok\" is INTJ (interjections like bravo, great etc)\n",
    "\n",
    "\n",
    "#     rule6_pairs = []\n",
    "#     for token in doc:\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         if(token.pos_ == \"INTJ\" and not token.is_stop):\n",
    "#             for child in children :\n",
    "#                 if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "#                     M = token.text\n",
    "#                     if child.idx in ner_heads:\n",
    "#                         A = ner_heads[child.idx].text\n",
    "#                     else:\n",
    "#                         A = child.text\n",
    "#                     # check_spelling(child.text)\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule6_pairs.append((A, M,sid.polarity_scores(M)['compound'],6))\n",
    "\n",
    "\n",
    "#     ## SEVENTH RULE OF DEPENDANCY PARSE -\n",
    "#     ## M - Sentiment modifier || A - Aspect\n",
    "#     ## ATTR - link between a verb like 'be/seem/appear' and its complement\n",
    "#     ## Example: 'this is garbage' -> (this, garbage)\n",
    "\n",
    "#     rule7_pairs = []\n",
    "#     for token in doc:\n",
    "#         children = token.children\n",
    "#         A = \"999999\"\n",
    "#         M = \"999999\"\n",
    "#         add_neg_pfx = False\n",
    "#         for child in children :\n",
    "#             if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "#                 if child.idx in ner_heads:\n",
    "#                     A = ner_heads[child.idx].text\n",
    "#                 else:\n",
    "#                     A = child.text\n",
    "#                 # check_spelling(child.text)\n",
    "\n",
    "#             if((child.dep_ == \"attr\") and not child.is_stop):\n",
    "#                 M = child.text\n",
    "#                 #check_spelling(child.text)\n",
    "\n",
    "#             if(child.dep_ == \"neg\"):\n",
    "#                 neg_prefix = child.text\n",
    "#                 add_neg_pfx = True\n",
    "\n",
    "#         if (add_neg_pfx and M != \"999999\"):\n",
    "#             M = neg_prefix + \" \" + M\n",
    "\n",
    "#         if(A != \"999999\" and M != \"999999\"):\n",
    "#             rule7_pairs.append((A, M,sid.polarity_scores(M)['compound'],7))\n",
    "\n",
    "\n",
    "\n",
    "#     aspects = []\n",
    "\n",
    "#     aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs + rule6_pairs + rule7_pairs\n",
    "#     prod_pronouns = ['it', 'this', 'they']\n",
    "\n",
    "#     # replace all instances of \"it\", \"this\" and \"they\" with \"product\"\n",
    "#     aspects = [(A,M,P,r) if A not in prod_pronouns else (\"product\",M,P,r) for A,M,P,r in aspects]\n",
    "\n",
    "# #     dic = {\"review_id\" : review_id , \"aspect_pairs\" : aspects, \"review_marketplace\" : review_marketplace\n",
    "# #     , \"customer_id\" : customer_id, \"product_id\" : product_id, \"product_parent\" : product_parent,\n",
    "# #     \"product_title\" : product_title, \"product_category\" : product_category, \"date\" : date, \"star_rating\" : star_rating, \"url\" : url}\n",
    "    \n",
    "    \n",
    "#     return aspects\n",
    "\n",
    "\n",
    "\n",
    "# def remove_digits(x):\n",
    "#     return \" \".join([t for t in x.split() if not t.isdigit()])\n",
    "\n",
    "\n",
    "\n",
    "# def get_word_vectors(unique_aspects, nlp=nlp):\n",
    "#     asp_vectors = []\n",
    "#     for aspect in unique_aspects:\n",
    "#         # print(aspect)\n",
    "#         token = nlp(aspect)\n",
    "#         asp_vectors.append(token.vector)\n",
    "#     return asp_vectors\n",
    "\n",
    "\n",
    "# def get_aspect_freq_map(aspects):\n",
    "#     aspect_freq_map = defaultdict(int)\n",
    "#     for asp in aspects:\n",
    "#         aspect_freq_map[asp] += 1\n",
    "#     return aspect_freq_map\n",
    "\n",
    "\n",
    "\n",
    "# NUM_CLUSTERS = 4\n",
    "\n",
    "# def get_word_cluster_labels(unique_aspects, nlp=nlp):\n",
    "#     # print(\"Found {} unique aspects for this product\".format(len(unique_aspects)))\n",
    "#     asp_vectors = get_word_vectors(unique_aspects, nlp)\n",
    "#     # n_clusters = min(NUM_CLUSTERS,len(unique_aspects))\n",
    "#     if len(unique_aspects) <= NUM_CLUSTERS:\n",
    "#         # print(\"Too few aspects ({}) found. No clustering required...\".format(len(unique_aspects)))\n",
    "#         return list(range(len(unique_aspects)))\n",
    "\n",
    "#     # print(\"Running k-means clustering...\")\n",
    "#     n_clusters = NUM_CLUSTERS\n",
    "#     kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
    "#     kmeans.fit(asp_vectors)\n",
    "#     labels = kmeans.labels_\n",
    "#     # dbscan = cluster.DBSCAN(eps = 0.2, min_samples = 2).fit(asp_vectors)\n",
    "#     # labels = dbscan.labels_\n",
    "\n",
    "#     # print(\"Finished running k-means clustering with {} labels\".format(len(labels)))\n",
    "#     # print(labels)\n",
    "#     return labels\n",
    "\n",
    "\n",
    "\n",
    "# def get_cluster_names_map(asp_to_cluster_map, aspect_freq_map):\n",
    "#     cluster_id_to_name_map = defaultdict()\n",
    "#     # cluster_to_asp_map = defaultdict()\n",
    "#     clusters = set(asp_to_cluster_map.values())\n",
    "#     for i in clusters:\n",
    "#         this_cluster_asp = [k for k,v in asp_to_cluster_map.items() if v == i]\n",
    "#         filt_freq_map = {k:v for k,v in aspect_freq_map.items() if k in this_cluster_asp}\n",
    "#         filt_freq_map = sorted(filt_freq_map.items(), key = lambda x: x[1], reverse = True)\n",
    "#         cluster_id_to_name_map[i] = filt_freq_map\n",
    "\n",
    "#         # cluster_to_asp_map[i] = this_cluster_asp\n",
    "\n",
    "#     # print(cluster_to_asp_map)\n",
    "#     # print(cluster_id_to_name_map)\n",
    "#     return cluster_id_to_name_map\n",
    "\n",
    "# def master_preprocess(df):\n",
    "#     df['wordcounts'] = df['review_body'].apply(lambda x: pp.get_wordcounts(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_emails(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_urls(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_html_tags(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_accented_chars(x))\n",
    "#     return df\n",
    "\n",
    "# # aspect_list = []\n",
    "# # for row in range(0,500):\n",
    "# #     aspect = apply_extraction(df3.iloc[row], nlp=nlp, sid=sid)\n",
    "# #     aspect_list.append(aspect)\n",
    "\n",
    "# # def remove_digits(x):\n",
    "# #     return \" \".join([t for t in x.split() if not t.isdigit()])\n",
    "\n",
    "# # def get_aspect_freq_map(aspects):\n",
    "# #     aspect_freq_map = defaultdict(int)\n",
    "# #     for asp in aspects:\n",
    "# #         aspect_freq_map[asp] += 1\n",
    "# #     return aspect_freq_map\n",
    "\n",
    "# # df2 = master_preprocess(df2)\n",
    "# # df2 = df2.loc[df2['wordcounts'] > 10].copy()\n",
    "# # df2.reset_index(drop=True, inplace=True)\n",
    "# # df2['aspect_tups'] = df2.apply(apply_extraction, axis=1)\n",
    "\n",
    "\n",
    "# # df_hp1 = master_preprocess(df_hp1)\n",
    "# # df_hp2 = master_preprocess(df_hp2)\n",
    "# # df_hp3 = master_preprocess(df_hp3)\n",
    "# # df_hp4 = master_preprocess(df_hp4)\n",
    "# # df_sb1 = master_preprocess(df_sb1)\n",
    "# # df_sb2 = master_preprocess(df_sb2)\n",
    "# # df_mp1 = master_preprocess(df_mp1)\n",
    "# # df_mp2 = master_preprocess(df_mp2)\n",
    "\n",
    "# def extract_aspect_sentiment_tuples(df):\n",
    "#     df = master_preprocess(df)\n",
    "#     df = df.loc[df['wordcounts'] > 10].copy()\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     df['aspect_tups'] = df.apply(apply_extraction, axis=1)\n",
    "#     df = df.explode('aspect_tups').copy()\n",
    "#     df.dropna(inplace=True)\n",
    "#     df['asp'] = df['aspect_tups'].apply(lambda x: x[0])\n",
    "#     df['modifier'] = df['aspect_tups'].apply(lambda x: x[1])\n",
    "#     df['modifier_sentiment'] = df['aspect_tups'].apply(lambda x: x[2])\n",
    "#     df['rule_number'] = df['aspect_tups'].apply(lambda x: x[3])\n",
    "#     return df\n",
    "\n",
    "# df_senti = extract_aspect_sentiment_tuples(df2)\n",
    "\n",
    "# df_senti.head()\n",
    "\n",
    "# def get_cluster_name_inputs(df):\n",
    "#     print('loading....')\n",
    "#     aspect_freq_map = get_aspect_freq_map(df['asp'].values)\n",
    "#     unique_asp_array = df['asp'].unique()\n",
    "#     mapped_labels = get_word_cluster_labels(unique_asp_array)\n",
    "#     asp_labels_map = dict(zip(unique_asp_array, mapped_labels))\n",
    "#     label_names_map = get_cluster_names_map(asp_labels_map, aspect_freq_map)\n",
    "    \n",
    "#     df['asp_cluster_label'] = df['asp'].map(asp_labels_map)\n",
    "    \n",
    "#     print(\"write misc if low counts and special characters\")\n",
    "#     print(\"the top word is usually the best fit\")\n",
    "   \n",
    "#     display(label_names_map[0][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_0 = input()\n",
    "\n",
    "#     display(label_names_map[1][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_1 = input()\n",
    "\n",
    "#     display(label_names_map[2][:10])\n",
    "  \n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_2 = input()\n",
    "\n",
    "#     display(label_names_map[3][:10])\n",
    "#     print(\"Pick a category for above words: \")\n",
    "#     clust_3 = input()\n",
    "\n",
    "#     clusters = [clust_0] + [clust_1] + [clust_2] + [clust_3]\n",
    "\n",
    "\n",
    "#     name_clust_dict = {0: clusters[0],\n",
    "#                        1: clusters[1],\n",
    "#                        2: clusters[2],\n",
    "#                        3: clusters[3]\n",
    "#                       }\n",
    "\n",
    "    \n",
    "\n",
    "#     df['cluster_name'] = df['asp_cluster_label'].map(name_clust_dict)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# df_senti = get_cluster_name_inputs(df_senti)\n",
    "\n",
    "# df_senti.head()\n",
    "\n",
    "# star_clusters = df_senti.groupby(['cluster_name', 'star_rating'])['star_rating'].count()\n",
    "\n",
    "# df_senti.loc[df_senti['modifier_sentiment'] == 0, 'polarity'] = 'neutral'\n",
    "# df_senti.loc[df_senti['modifier_sentiment'] > 0, 'polarity'] = 'positive'\n",
    "# df_senti.loc[df_senti['modifier_sentiment'] < 0, 'polarity'] = 'negative'\n",
    "\n",
    "# polarity_clusters = df_senti.groupby(['cluster_name', 'polarity'])['polarity'].count()\n",
    "\n",
    "# polarity_clusters.plot(kind='bar')\n",
    "\n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# polarity_clusters.plot(kind='bar')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"http://www.amazon.com/exec/obidos/ASIN/\" + \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_amazonlink(product_id):\n",
    "#     url = \"http://www.amazon.com/exec/obidos/ASIN/\" + product_id\n",
    "#     return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_amazonlink(\"B00JOQIO6S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def master_preprocess(df):\n",
    "#     df['wordcounts'] = df['review_body'].apply(lambda x: pp.get_wordcounts(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_emails(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_urls(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_html_tags(x))\n",
    "#     df['review_body'] = df['review_body'].apply(lambda x: pp.remove_accented_chars(x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect_list = []\n",
    "# for row in range(0,500):\n",
    "#     aspect = apply_extraction(df3.iloc[row], nlp=nlp, sid=sid)\n",
    "#     aspect_list.append(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_digits(x):\n",
    "#     return \" \".join([t for t in x.split() if not t.isdigit()])\n",
    "\n",
    "# def get_aspect_freq_map(aspects):\n",
    "#     aspect_freq_map = defaultdict(int)\n",
    "#     for asp in aspects:\n",
    "#         aspect_freq_map[asp] += 1\n",
    "#     return aspect_freq_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mention_tags(x): \n",
    "#     mention_tag = [t for t in x.split() if t.startswith('@')]\n",
    "#     return ' '.join(mention_tag)\n",
    "\n",
    "# def remove_mention_tags(x): \n",
    "#     no_mention = [t for t in x.split() if not t.startswith('@')]\n",
    "#     return ' '.join(no_mention)\n",
    "\n",
    "# def get_value_counts2(df, col):\n",
    "#     text = ' '.join(df[col])\n",
    "#     text = text.split()\n",
    "#     freq = pd.Series(text).value_counts()\n",
    "#     return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered.to_csv('data/filtered_twitter_support.csv')\n",
    "# df_apple.to_csv('data/apple_support_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2 = \"ありがとうございます\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_apple_sample = df_apple.sample(3000)\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.get_expan(x))\n",
    "# df_apple_sample['emails'] = df_apple_sample['text'].apply(lambda x: pp.get_emails(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_emails(x))\n",
    "# df_apple_sample['urls'] = df_apple_sample['text'].apply(lambda x: pp.get_urls(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_urls(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_rt(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: remove_mention_tags(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_special_chars(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_html_tags(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_accented_chars(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_apple_sample = df_apple_sample.loc[df_apple_sample['word_counts'] >= 10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqy = get_value_counts2(df_apple_sample, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = \"Apple is looking at buying U.K. startup for $1 billion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_POS_tags(x):\n",
    "#     doc = nlp(x)\n",
    "#     for token in doc:\n",
    "#         print(token.text,token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# files = glob.glob(\"new_data/*.xlsx\")\n",
    "# dfs = [pd.read_excel(f) for f in files]\n",
    "\n",
    "# data = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(columns=['bage', \n",
    "#                    'url',\n",
    "#                    'author_descriptor', \n",
    "#                    'author_profile_img',\n",
    "#                    'author_title', \n",
    "#                    'author_url', \n",
    "#                    'id',\n",
    "#                    'official_comment_banner', \n",
    "#                     'query', \n",
    "#                    'rating_text', \n",
    "#                    'title',\n",
    "#                    'helpful',\n",
    "#                    'variation'],\n",
    "#           inplace = True\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(data.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_star_df_undersample = data.loc[data['rating'] == 5].sample(2000)\n",
    "# five_star_df_undersample = five_star_df_undersample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_star_index = data.loc[data['rating'] == 5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_five_stars_df = data.drop(five_star_index)\n",
    "# no_five_stars_df = no_five_stars_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced = pd.concat([no_five_stars_df, five_star_df_undersample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(df_balanced.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced['word_count'] = df_balanced['body'].apply(lambda x: pp.get_wordcounts)\n",
    "\n",
    "# df_apple_sample['emails'] = df_apple_sample['text'].apply(lambda x: pp.get_emails(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_emails(x))\n",
    "# df_apple_sample['urls'] = df_apple_sample['text'].apply(lambda x: pp.get_urls(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_urls(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_rt(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_special_chars(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_html_tags(x))\n",
    "# df_apple_sample['text'] = df_apple_sample['text'].apply(lambda x: pp.remove_accented_chars(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced['word_count'] = df_balanced['body'].apply(lambda x: pp.get_wordcounts(x))\n",
    "   \n",
    "# df_balanced['char_count'] = df_balanced['body'].apply(lambda x: pp.get_charcounts(x))\n",
    "\n",
    "# df_balanced['avg_wordlen'] = df_balanced['body'].apply(lambda x: pp.get_avg_wordlen(x))\n",
    "\n",
    "# df_balanced['stopword_count'] = df_balanced['body'].apply(lambda x: pp.get_stopwords_counts(x))\n",
    "\n",
    "# df_balanced['digit_count'] = df_balanced['body'].apply(lambda x: pp.get_digit_counts(x))\n",
    "    \n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.get_expan(x))\n",
    "\n",
    "# df_balanced['emails'] = df_balanced['body'].apply(lambda x: pp.get_emails(x))\n",
    "\n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.remove_emails(x))\n",
    "    \n",
    "# df_balanced['urls'] = df_balanced['body'].apply(lambda x: pp.get_urls(x))\n",
    "\n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.remove_urls(x))\n",
    "\n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.remove_special_chars(x))\n",
    "\n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.remove_html_tags(x))\n",
    "\n",
    "# df_balanced['body'] = df_balanced['body'].apply(lambda x: pp.remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df_balanced.loc[df_balanced['rating'] > 1].copy()\n",
    "# df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df3['body'].sample(1).values[0]\n",
    "# x2 = df3['body'].sample(1).values[0]\n",
    "# x3 = df3['body'].sample(1).values[0]\n",
    "# x4 = df3['body'].sample(1).values[0]\n",
    "# x5 = df3['body'].sample(1).values[0]\n",
    "# x6 = df3['body'].sample(1).values[0]\n",
    "# x7 = df3['body'].sample(1).values[0]\n",
    "# x8 = df3['body'].sample(1).values[0]\n",
    "# x9 = df3['body'].sample(1).values[0]\n",
    "# x10 = df3['body'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x, '\\n\\n',\n",
    "#       x2, '\\n\\n',\n",
    "#       x3, '\\n\\n',\n",
    "#       x4, '\\n\\n',\n",
    "#       x5, '\\n\\n',\n",
    "#       x6, '\\n\\n',\n",
    "#       x7, '\\n\\n',\n",
    "#       x8, '\\n\\n',\n",
    "#       x9, '\\n\\n',\n",
    "#       x10, '\\n\\n',\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# aspects = []\n",
    "# doc = nlp(x5)\n",
    "# doc_index = 0\n",
    "\n",
    "\n",
    "# # for token in doc:\n",
    "# #     print(token.lemma_, token.tag_, token.dep_,)\n",
    "\n",
    "# for tok in doc:\n",
    "#     if (tok.pos_ == 'NOUN' and doc[doc_index - 1].pos_ == 'ADJ'):\n",
    "#             aspects.append(str(doc[doc_index - 1]) + \" \" + str(tok))                    \n",
    "#     doc_index = doc_index + 1\n",
    "\n",
    "# print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in sentences:\n",
    "#     doc = nlp(x5)\n",
    "#     descriptive_term = ''\n",
    "#     for token in doc:\n",
    "#         if token.pos_ == 'ADJ':\n",
    "#             descriptive_term = token\n",
    "#     print(x5)\n",
    "#     print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspects = []\n",
    "# def get_aspect_sentiment_pairs(x):\n",
    "#     doc = nlp(x)\n",
    "#     descriptive_term = ''\n",
    "#     target = ''\n",
    "#     for token in doc:\n",
    "#         if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "#             target = token.text\n",
    "#         if token.pos_ == 'ADJ':\n",
    "#             prepend = ''\n",
    "#             for child in token.children:\n",
    "#                 if child.pos_ != 'ADV':\n",
    "#                     continue\n",
    "#                 prepend += child.text + ' '\n",
    "#             descriptive_term = prepend + token.text\n",
    "#     aspects.append({'aspect': target,\n",
    "#                     'description': descriptive_term})\n",
    "#     return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get_aspect_sentiment_pairs(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(x5)\n",
    "# for token in doc:\n",
    "#     print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "#             [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('amazon_review_full_csv/train.csv',\n",
    "#                  names = ['rating', 'title', 'review']   \n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.sample(1000000).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.loc[(df2['rating'] == 4) | (df2['rating'] == 5), 'sentiment'] = 'positive'\n",
    "# df2.loc[(df2['rating'] == 3), 'sentiment'] = 'neutral'\n",
    "# df2.loc[(df2['rating'] == 1) | (df2['rating'] == 2), 'sentiment'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df.loc[df['rating'] == 3].sample(200010).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df2,df3])\n",
    "# df.loc[(df2['rating'] == 3), 'sentiment'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import gzip\n",
    "\n",
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'rb')\n",
    "#     for l in g:\n",
    "#         yield json.loads(l)\n",
    "\n",
    "# def getDF(path):\n",
    "#     i = 0\n",
    "#     df = {}\n",
    "#     for d in parse(path):\n",
    "#         df[i] = d\n",
    "#         i += 1\n",
    "#     return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "# df = getDF('new_data/Electronics.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = pd.read_csv('new_data/amazon_review_polarity_csv/train.csv',\n",
    "#                   names=['sentiment', 'title', 'text']\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5 = df4.sample(n=100000, weights='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
